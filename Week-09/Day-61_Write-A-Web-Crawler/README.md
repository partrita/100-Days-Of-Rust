# 웹 크롤러 작성

우리 대부분은 GoogleBot과 같은 웹 스파이더 및 크롤러에 익숙합니다. 이러한 크롤러는 웹 페이지를 방문하여 콘텐츠를 인덱싱한 다음 해당 페이지의 외부 링크를 방문합니다. 크롤러는 지속적으로 개발되는 흥미로운 기술입니다.

웹 크롤러는 큐잉과 HTML 구문 분석을 결합하여 검색 엔진 등의 기반을 형성합니다. 간단한 크롤러를 작성하는 것은 몇 가지를 함께 사용하는 좋은 연습입니다. 잘 작동하는 크롤러를 작성하는 것은 또 다른 단계입니다.

이 과제에서는 원하는 단일 샷 웹 클라이언트(예: Python의 httplib 또는 여러 libcurl 바인딩 중 하나)를 사용할 수 있습니다. Mechanize와 같은 크롤링 라이브러리는 사용할 수 없습니다. BeautifulSoup와 같은 HTML 구문 분석 라이브러리를 사용할 수 있습니다. PhantomJS와 같은 헤드리스 브라우저는 사용할 수 없습니다. 이 과제의 목적은 페이지 가져오기, 링크 재조립, 링크 검색 및 조립, 큐에 추가, 큐 깊이 관리, 합리적인 순서로 방문(중복 방문 방지)을 함께 연결하는 것입니다.

크롤러는 다음 기능을 지원해야 합니다.

- HTTP/1.1 클라이언트 동작
- GET 요청만 지원해야 합니다.
- HTML에 표시된 모든 링크(앵커, 이미지, 스크립트 등) 구문 분석
- 세션당 동일한 링크를 두 번 이상 방문하지 마십시오.

선택적 기능에는 HTTPS 지원, robots.txt 지원, 크롤러를 제한하는 도메인 지원, 결과 저장(예: wget 방식)이 포함됩니다.

크롤링하는 내용에 주의하십시오! 인터넷에서 차단당하지 마십시오. 속도 제한 및 원치 않는 방문자를 식별하는 기타 메커니즘이 트리거될 수 있으므로 제어하는 로컬 서버를 크롤링하는 것이 좋습니다.

## 입력 설명

최소 두 개의 매개변수를 사용합니다.
- 시작 (시드) URL
- 재귀할 최대 깊이 (예: "1"은 HTML 페이지와 관련된 모든 리소스(이미지 및 스크립트 등)를 가져오지만 외부 앵커 링크는 방문하지 않음, 깊이 "2"는 해당 첫 번째 페이지에서 찾은 앵커 링크만 방문함 등)

## 출력 설명

검색된 URL의 배열/벡터입니다.